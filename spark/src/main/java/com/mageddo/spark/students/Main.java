package com.mageddo.spark.students;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.mageddo.spark.students.vo.Student;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.OutputStream;
import java.nio.file.Files;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.Statement;
import java.util.*;

import static com.mageddo.spark.students.DBUtils.getConnection;

public class Main {

	private static final ObjectMapper MAPPER = new ObjectMapper();

	public static void main(String[] args) throws Exception {

		System.setProperty("java.awt.headless", "false");
		org.hsqldb.util.DatabaseManagerSwing.main(new String[] {
			"--url",  "jdbc:hsqldb:mem:testdb", "--noexit"
		});

		try(Connection db = DBUtils.getConnection()){
			try(Statement stm = db.createStatement()){
				stm.execute("CREATE TABLE SCHOOL (\n" +
					"\tID INTEGER GENERATED BY DEFAULT AS IDENTITY(START WITH 1, INCREMENT BY 1) PRIMARY KEY,\n" +
					"\tNAME VARCHAR(255)\n" +
					")");
				stm.execute("CREATE TABLE STUDENT (\n" +
					"\tID INTEGER GENERATED BY DEFAULT AS IDENTITY(START WITH 1, INCREMENT BY 1) PRIMARY KEY,\n" +
					"\tSCHOOL_ID INTEGER,\n" +
					"\tNAME VARCHAR(255)\n" +
					")");
			}
			db.commit();
		}

		final File jsonFile = Files.createTempFile("spark", ".tmp").toFile();
		final int students = 1_000_000;
		try(OutputStream out = new BufferedOutputStream(new FileOutputStream(jsonFile))){
			for(int i = students; i > 0; i--){
				final Student student = new Student(String.valueOf(new Random().nextInt(students)), String.valueOf(new Random().nextInt(students / 2)));
				out.write(getMapper().writeValueAsBytes(student));
				out.write('\n');
			}
		}

		final JavaSparkContext sc = createContext();
		final JavaPairRDD<Student, Student> studentsPair = sc.textFile(jsonFile.getAbsolutePath())
		.mapToPair(line -> {
			final Student student = getMapper().readValue(line, Student.class);
			student.name = String.valueOf(System.nanoTime());
			return new Tuple2<>(student, student);
		});

		final JavaPairRDD<Student, Student> savedSchools = studentsPair
		.distinct()
		.mapPartitionsToPair(it -> {

			final Set<Tuple2<Student, Student>> schools = new HashSet<>();
			try(Connection conn = getConnection()){

				it.forEachRemaining(tuple -> {

					try(PreparedStatement stm = conn.prepareStatement("INSERT INTO SCHOOL (NAME) VALUES (?)", Statement.RETURN_GENERATED_KEYS)){

						stm.setString(1, tuple._1.schoolName);
						stm.executeUpdate();

						try(ResultSet rs = stm.getGeneratedKeys()){
							if(rs.next()){
								tuple._1.id = rs.getInt(1);
							}else {
								throw new IllegalStateException();
							}
						}
						schools.add(new Tuple2<>(tuple._1, tuple._2));

					}catch (Exception e){
						throw new RuntimeException(e);
					}
				});
				conn.commit();
			}
			return schools.iterator();
		});

		savedSchools.join(studentsPair)
		.foreachPartition(it -> {
			try(Connection conn = getConnection()){

				it.forEachRemaining(tuple -> {

					try(PreparedStatement stm = conn.prepareStatement("INSERT INTO STUDENT (SCHOOL_ID, NAME) VALUES (?, ?)")){

						stm.setInt(1, tuple._1.id);
						stm.setString(2, tuple._2._2.name);
						stm.executeUpdate();

					}catch (Exception e){
						throw new RuntimeException(e);
					}

				});
				conn.commit();
			}

		});

		sc.close();

	}

	static JavaSparkContext createContext() {
		final SparkConf sparkConf = new SparkConf()
			.setAppName("WordCount")
			.setMaster("local");
		final JavaSparkContext sc = new JavaSparkContext(sparkConf);
		sc.setLogLevel("ERROR");
		return sc;
	}

	static ObjectMapper getMapper() {
		return MAPPER;
	}
}
